{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbc34d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Download required NLTK data\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "# Load JSONL files\n",
    "def load_jsonl(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line.strip()))\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Load your files\n",
    "print(\"Loading JSONL files...\")\n",
    "model_summaries = load_jsonl('test_summaries.jsonl')  # Change to your file path\n",
    "gold_summaries = load_jsonl('test_ref_summ.jsonl')    # Change to your file path\n",
    "\n",
    "print(f\"✓ Loaded {len(model_summaries)} model summaries\")\n",
    "print(f\"✓ Loaded {len(gold_summaries)} gold summaries\")\n",
    "\n",
    "# Ensure both files have the same IDs and are aligned\n",
    "if 'ID' in model_summaries.columns and 'ID' in gold_summaries.columns:\n",
    "    model_summaries = model_summaries.sort_values('ID').reset_index(drop=True)\n",
    "    gold_summaries = gold_summaries.sort_values('ID').reset_index(drop=True)\n",
    "    print(\"✓ Aligned summaries by ID\")\n",
    "\n",
    "# Extract summary texts (adjust column names as needed)\n",
    "model_texts = model_summaries['Summary'].tolist()  # Change 'Summary' to your column name\n",
    "gold_texts = gold_summaries['reference_summary'].tolist()    # Change 'Summary' to your column name\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"Evaluating summaries...\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Initialize ROUGE scorer\n",
    "scorer = rouge_scorer.RougeScorer(['rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "# Initialize smoothing for BLEU (handles edge cases)\n",
    "smoothing = SmoothingFunction().method1\n",
    "\n",
    "# Store scores for each summary\n",
    "rouge2_scores = []\n",
    "rougeL_scores = []\n",
    "bleu_scores = []\n",
    "\n",
    "# Calculate scores for each summary pair\n",
    "for i, (model_text, gold_text) in enumerate(zip(model_texts, gold_texts)):\n",
    "    # ROUGE scores\n",
    "    rouge_results = scorer.score(gold_text, model_text)\n",
    "    rouge2_scores.append(rouge_results['rouge2'].fmeasure)\n",
    "    rougeL_scores.append(rouge_results['rougeL'].fmeasure)\n",
    "    \n",
    "    # BLEU score\n",
    "    reference = [word_tokenize(gold_text.lower())]\n",
    "    hypothesis = word_tokenize(model_text.lower())\n",
    "    bleu = sentence_bleu(reference, hypothesis, smoothing_function=smoothing)\n",
    "    bleu_scores.append(bleu)\n",
    "    \n",
    "    # Print progress every 100 summaries\n",
    "    if (i + 1) % 100 == 0:\n",
    "        print(f\"Processed {i + 1}/{len(model_texts)} summaries...\")\n",
    "\n",
    "print(f\"✓ Evaluation complete!\")\n",
    "\n",
    "# Calculate average scores\n",
    "avg_rouge2 = np.mean(rouge2_scores)\n",
    "avg_rougeL = np.mean(rougeL_scores)\n",
    "avg_bleu = np.mean(bleu_scores)\n",
    "avg_overall = (avg_rouge2 + avg_rougeL + avg_bleu) / 3\n",
    "\n",
    "# Calculate standard deviations\n",
    "std_rouge2 = np.std(rouge2_scores)\n",
    "std_rougeL = np.std(rougeL_scores)\n",
    "std_bleu = np.std(bleu_scores)\n",
    "\n",
    "# Print results\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"EVALUATION RESULTS\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"\\n Average Scores:\")\n",
    "print(f\"  • ROUGE-2:      {avg_rouge2:.4f} (±{std_rouge2:.4f})\")\n",
    "print(f\"  • ROUGE-L:      {avg_rougeL:.4f} (±{std_rougeL:.4f})\")\n",
    "print(f\"  • BLEU:         {avg_bleu:.4f} (±{std_bleu:.4f})\")\n",
    "print(f\"  • Overall Avg:  {avg_overall:.4f}\")\n",
    "\n",
    "print(f\"\\n Score Ranges:\")\n",
    "print(f\"  • ROUGE-2:  [{min(rouge2_scores):.4f} - {max(rouge2_scores):.4f}]\")\n",
    "print(f\"  • ROUGE-L:  [{min(rougeL_scores):.4f} - {max(rougeL_scores):.4f}]\")\n",
    "print(f\"  • BLEU:     [{min(bleu_scores):.4f} - {max(bleu_scores):.4f}]\")\n",
    "\n",
    "print(f\"\\n Percentiles:\")\n",
    "print(f\"  • ROUGE-2:  25th={np.percentile(rouge2_scores, 25):.4f}, \"\n",
    "      f\"50th={np.percentile(rouge2_scores, 50):.4f}, \"\n",
    "      f\"75th={np.percentile(rouge2_scores, 75):.4f}\")\n",
    "print(f\"  • ROUGE-L:  25th={np.percentile(rougeL_scores, 25):.4f}, \"\n",
    "      f\"50th={np.percentile(rougeL_scores, 50):.4f}, \"\n",
    "      f\"75th={np.percentile(rougeL_scores, 75):.4f}\")\n",
    "print(f\"  • BLEU:     25th={np.percentile(bleu_scores, 25):.4f}, \"\n",
    "      f\"50th={np.percentile(bleu_scores, 50):.4f}, \"\n",
    "      f\"75th={np.percentile(bleu_scores, 75):.4f}\")\n",
    "\n",
    "# Create detailed results DataFrame\n",
    "results_df = pd.DataFrame({\n",
    "    'ID': model_summaries['ID'] if 'ID' in model_summaries.columns else range(len(model_texts)),\n",
    "    'ROUGE-2': rouge2_scores,\n",
    "    'ROUGE-L': rougeL_scores,\n",
    "    'BLEU': bleu_scores,\n",
    "    'Average': [(r2 + rl + b) / 3 for r2, rl, b in zip(rouge2_scores, rougeL_scores, bleu_scores)]\n",
    "})\n",
    "\n",
    "# Save detailed results\n",
    "results_df.to_csv('evaluation_results.csv', index=False)\n",
    "print(f\"\\n Detailed results saved to: evaluation_results.csv\")\n",
    "\n",
    "# Save summary statistics\n",
    "summary_stats = pd.DataFrame({\n",
    "    'Metric': ['ROUGE-2', 'ROUGE-L', 'BLEU', 'Overall Average'],\n",
    "    'Mean': [avg_rouge2, avg_rougeL, avg_bleu, avg_overall],\n",
    "    'Std': [std_rouge2, std_rougeL, std_bleu, 0],\n",
    "    'Min': [min(rouge2_scores), min(rougeL_scores), min(bleu_scores), 0],\n",
    "    'Max': [max(rouge2_scores), max(rougeL_scores), max(bleu_scores), 0],\n",
    "    '25th': [np.percentile(rouge2_scores, 25), np.percentile(rougeL_scores, 25), \n",
    "             np.percentile(bleu_scores, 25), 0],\n",
    "    '50th': [np.percentile(rouge2_scores, 50), np.percentile(rougeL_scores, 50), \n",
    "             np.percentile(bleu_scores, 50), 0],\n",
    "    '75th': [np.percentile(rouge2_scores, 75), np.percentile(rougeL_scores, 75), \n",
    "             np.percentile(bleu_scores, 75), 0]\n",
    "})\n",
    "\n",
    "summary_stats.to_csv('summary_statistics.csv', index=False)\n",
    "print(f\" Summary statistics saved to: summary_statistics.csv\")\n",
    "\n",
    "# Find best and worst performing summaries\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"TOP 5 BEST SUMMARIES (by average score):\")\n",
    "print(f\"{'='*70}\")\n",
    "top5 = results_df.nlargest(5, 'Average')\n",
    "for idx, row in top5.iterrows():\n",
    "    print(f\"ID: {row['ID']}, Avg: {row['Average']:.4f}, \"\n",
    "          f\"R-2: {row['ROUGE-2']:.4f}, R-L: {row['ROUGE-L']:.4f}, BLEU: {row['BLEU']:.4f}\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"TOP 5 WORST SUMMARIES (by average score):\")\n",
    "print(f\"{'='*70}\")\n",
    "bottom5 = results_df.nsmallest(5, 'Average')\n",
    "for idx, row in bottom5.iterrows():\n",
    "    print(f\"ID: {row['ID']}, Avg: {row['Average']:.4f}, \"\n",
    "          f\"R-2: {row['ROUGE-2']:.4f}, R-L: {row['ROUGE-L']:.4f}, BLEU: {row['BLEU']:.4f}\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"✓ Evaluation complete!\")\n",
    "print(f\"{'='*70}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
